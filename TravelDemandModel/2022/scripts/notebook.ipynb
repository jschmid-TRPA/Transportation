{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import arcpy\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# pandas options\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "# current working directory\n",
    "local_path = pathlib.Path().absolute()\n",
    "# set data path as a subfolder of the current working directory TravelDemandModel\\2022\\\n",
    "data_dir = local_path.parents[0] / 'data/raw_data'\n",
    "# folder to save processed data\n",
    "out_dir  = local_path.parents[0] / 'data/processed_data'\n",
    "# workspace gdb for stuff that doesnt work in memory\n",
    "gdb = os.path.join(local_path,'Workspace.gdb')\n",
    "# set environement workspace to in memory \n",
    "arcpy.env.workspace = 'memory'\n",
    "# clear memory workspace\n",
    "arcpy.management.Delete('memory')\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "# schema for the final output\n",
    "final_schema = ['APN', 'Residential_Units', 'TouristAccommodation_Units', 'CommercialFloorArea_SqFt',\n",
    "                'RoomsRented_PerDay', 'VHR_Lodging_Occupancy_Rate','TAU_Lodging_Occupancy_Rate', \n",
    "                'PrimaryResidence_Rate', 'SecondaryResidence_Rate',\n",
    "                'HighIncome_Rate',\t'MediumIncome_Rate', 'LowIncome_Rate', 'PersonsPerUnit',\n",
    "                'TAU_TYPE', 'VHR', 'BLOCK_GROUP', 'TAZ', 'OCCUPANCY_ZONE', \n",
    "                'JURISDICTION', 'COUNTY', 'OWNERSHIP_TYPE','EXISTING_LANDUSE', 'WITHIN_TRPA_BNDY', \n",
    "                'PARCEL_ACRES', 'PARCEL_SQFT', 'SHAPE']\n",
    "\n",
    "# pickle variables for each part\n",
    "# part 1 - spatial join categories, occupancy rates, and parcels\n",
    "sdfParcel_pickle_part1 = out_dir / 'sdfParcel_pickle1.pkl'\n",
    "# part 2 - known occupancy rates applied to the parcel and spatial interpolation of occupancy rates\n",
    "sdfParcel_pickle_part2 = out_dir / 'sdfParcel_pickle2.pkl'\n",
    "# part 3 - fill in missing occupancy rates with spatial interpolation\n",
    "sdfParcel_pickle_part3 = out_dir / 'sdfParcel_pickle3.pkl'\n",
    "# part 4 - \n",
    "sdfParcel_pickle_part4 = out_dir / 'sdfParcel_pickle4.pkl'\n",
    "# part 5 - \n",
    "sdfParcel_pickle_part5 = out_dir / 'sdfParcel_pickle5.pkl'\n",
    "# part 6 - \n",
    "sdfParcel_pickle_part6 = out_dir / 'sdfParcel_pickle6.pkl'\n",
    "# pickle for occupancry rates\n",
    "occupancy_rates_pickle = out_dir / 'occupancy_rates.pkl'\n",
    "# campground pickles\n",
    "campground_pickle = out_dir / 'campground.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Future Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to do a spatial join and\n",
    "# map values from the source to the target\n",
    "def spatial_join_map(target, source, join_field,  map_field, target_field):\n",
    "    # spatial join\n",
    "    arcpy.SpatialJoin_analysis(target, source, 'memory\\\\temp', \n",
    "                               'JOIN_ONE_TO_ONE', 'KEEP_ALL','HAVE_THEIR_CENTER_IN')\n",
    "    # get result as a spatial dataframe\n",
    "    join = pd.DataFrame.spatial.from_featureclass('memory\\\\temp')\n",
    "    join.info()\n",
    "    # map values\n",
    "    target[target_field] = target[join_field].map(dict(zip(join[join_field], join[map_field])))\n",
    "    return target\n",
    "\n",
    "# check for duplicates\n",
    "def check_dupes(df, col):\n",
    "    df['is_duplicate'] = df.duplicated(subset=col, keep=False)\n",
    "    df.is_duplicate.value_counts()\n",
    "    df.loc[df['is_duplicate'] == True]\n",
    "    df = df.drop_duplicates(subset=col, keep='first', inplace=True)\n",
    "    return df[df.duplicated([col], keep=False)]\n",
    "\n",
    "# check if field exists in data frame and final_schema and if not add it\n",
    "def check_field(df, fields):\n",
    "    for field in fields:\n",
    "        if field not in df.columns:\n",
    "            df[field] = np.nan\n",
    "    return df\n",
    "\n",
    "# function to run interpolation and join by APN\n",
    "def interpolate_join(df, sdf):\n",
    "    # interpolate occupancy rate for VHR and TAU parcels where NULL\n",
    "    return df\n",
    "\n",
    "# function to fill missing values\n",
    "def fill_missing_values(df, sdf):\n",
    "    return df\n",
    "\n",
    "# function to run zonal stats and map values\n",
    "def zonal_stats_map(target, source, join_field,  map_field, target_field):\n",
    "    # zonal stats\n",
    "    arcpy.sa.ZonalStatisticsAsTable(target, join_field, source, 'memory\\\\temp', 'DATA', 'MEAN')\n",
    "    # get result as a spatial dataframe\n",
    "    join = pd.DataFrame.spatial.from_featureclass('memory\\\\temp')\n",
    "    join.info()\n",
    "    # map values\n",
    "    target[target_field] = target[join_field].map(dict(zip(join[join_field], join[map_field]))\n",
    "    return target\n",
    "\n",
    "# save to pickle\n",
    "def to_pickle(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f'{filename} pickled')\n",
    "\n",
    "# save to pickle and feature class\n",
    "def to_pickle_fc(data, filename):\n",
    "    data.spatial.to_featureclass(filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f'{filename} pickled and saved as feature class')\n",
    "\n",
    "# get a pickled file as a dataframe\n",
    "def from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f'{filename} unpickled')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAZ feature layer polygons\n",
    "taz_url = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/6'\n",
    "# get as spatial dataframe\n",
    "sdf_taz = get_fs_data_spatial(taz_url)\n",
    "# set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sdf_taz.spatial.sr = sr\n",
    "\n",
    "# parcel development layer polygons\n",
    "units_url = 'https://maps.trpa.org/server/rest/services/Existing_Development/MapServer/2'\n",
    "# query 2022 rows\n",
    "sdf_units = get_fs_data_spatial_query(units_url, \"Year = 2022\")\n",
    "sdf_units.spatial.sr = sr\n",
    "\n",
    "# block group feature layer polygons\n",
    "block_groups_url = 'https://maps.trpa.org/server/rest/services/Demographics/MapServer/27'\n",
    "sdf_block = get_fs_data_spatial(block_groups_url)\n",
    "sdf_block = sdf_block.loc[(sdf_block['YEAR'] == 2020) & (sdf_block['GEOGRAPHY'] == 'Block Group')]\n",
    "sdf_block.spatial.sr = sr\n",
    "\n",
    "# vhr feature layer polygons \n",
    "vhr_url = 'https://maps.trpa.org/server/rest/services/VHR/MapServer/0'\n",
    "sdf_vhr = get_fs_data_spatial(vhr_url)\n",
    "sdf_vhr.spatial.sr = sr\n",
    "# filter vhr layer to active status\n",
    "sdf_vhr = sdf_vhr.loc[sdf_vhr['Status'] == 'Active']\n",
    "\n",
    "# ACS 2022 bolock group table\n",
    "census_url = 'https://maps.trpa.org/server/rest/services/Demographics/MapServer/28'\n",
    "df_census = get_fs_data(census_url)\n",
    "df_census_2022 = df_census.loc[(df_census['year_sample'] == 2022) & (df_census['sample_level'] == 'block group')]\n",
    "\n",
    "# campground points feature layer\n",
    "campground_url = 'https://maps.trpa.org/server/rest/services/Recreation/MapServer/1'\n",
    "sdf_campground =  get_fs_data_spatial_query(campground_url, \"RECREATION_TYPE='Campground'\")\n",
    "\n",
    "# campground visits table\n",
    "campvisits_url = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/14'\n",
    "dfCamp = get_fs_data_query(campvisits_url, \"Year = 2022\")\n",
    "\n",
    "# occupancy zone feature layer polygons\n",
    "occupancyzones_url = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/15'\n",
    "sdf_occ = get_fs_data_spatial(occupancyzones_url)\n",
    "sdf_occ.spatial.sr = sr\n",
    "\n",
    "# occupancy rate table\n",
    "occupancyrate_url = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/13'\n",
    "df_occ = get_fs_data(occupancyrate_url)\n",
    "\n",
    "# school enrollment table\n",
    "school_url_table     = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/17'\n",
    "df_school_enrollment = get_fs_data_query(school_url_table, \"Year = '2021-2022'\")\n",
    "\n",
    "# school feature layer points\n",
    "school_url_spatial    = 'https://maps.trpa.org/server/rest/services/Transportation_Planning/MapServer/16'\n",
    "sdf_school            =  get_fs_data_spatial(school_url_spatial)\n",
    "sdf_school.spatial.sr = sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in missing data\n",
    "* Washoe Rooms Available, Washoe Reported Occupancy Rates, Washoe Quarterly VHRs added to Monthly Rows\n",
    "* Rest of El Dorado County VHR zones need Rooms Available & Rooms Rented\n",
    "    * Why are we using CSLT rate data instead of IDW interpolated data?\n",
    "* Weight the rates and calculate rooms rented per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of occupancy rates table source data\n",
    "dfOcc = df_occ.copy()\n",
    "\n",
    "# filter to columns \n",
    "columns = ['Zone_ID', 'Period', 'RoomType', 'Report_OccRate','TRPA_OccRate']\n",
    "\n",
    "# dictinary to convert the time frames to make things cleaner\n",
    "timeframe_dict = {\n",
    "    '2022-06-01': 'June',\n",
    "    '2022-08-01': 'August',\n",
    "    '2022-09-01': 'September',\n",
    "    'Q4 21-22'  : 'April-June',\n",
    "    'Q1 22-23'  : 'July-September',\n",
    "    'Q2 2022'   : 'April-June',\n",
    "    'Q3 2022'   : 'July-September'\n",
    "}\n",
    "\n",
    "# Period field based on Timeframe and timeframe_dict\n",
    "dfOcc['Period'] = dfOcc['Timeframe'].map(timeframe_dict)\n",
    "\n",
    "## Fill in Missing Data for Washoe County ##\n",
    "\n",
    "# get total WA taus and vhrs from the parcel layer\n",
    "tauWA = sdfParcel.loc[(sdfParcel.COUNTY == 'WA'), 'TouristAccommodation_Units'].sum()\n",
    "vhrWA = sdfParcel.loc[(sdfParcel.COUNTY == 'WA')&(sdfParcel.VHR == 'Yes'), 'APN'].count()\n",
    "\n",
    "# Calculate Rooms available for HotelMotel, Casino, Resort in Washoe County using total TAUs from the parcel layer\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly') &\n",
    "                (dfOcc.RoomType.isin(['HotelMotel', 'Casino', 'Resort'])) & (dfOcc['Period'].isin(['June', 'September'])), \n",
    "                'Report_RoomsAvailable'] = tauWA * 30\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly') &\n",
    "              (dfOcc.RoomType.isin(['HotelMotel', 'Casino', 'Resort'])) & (dfOcc['Period'].isin(['July','August'])), \n",
    "              'Report_RoomsAvailable'] = tauWA * 31\n",
    "\n",
    "# caclulate Rooms available for VHRs in Washoe County using total VHRs from the parcel layer\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly') & \n",
    "              (dfOcc.RoomType == 'VHR') & (dfOcc['Period'].isin(['July', 'August'])), \n",
    "              'Report_RoomsAvailable'] = vhrWA * 31\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly') & \n",
    "              (dfOcc.RoomType == 'VHR') & (dfOcc['Period'].isin(['June', 'September'])), \n",
    "              'Report_RoomsAvailable'] = vhrWA * 30\n",
    "\n",
    "# if the Zone_ID is Washoe County and VHR and Timeframe is the Q3 or Q2 then /3 to get monthly rooms available for that quarter/row\n",
    "waVHRZoneQ2 = dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc.RoomType == 'VHR') & (dfOcc.Timeframe == 'Q2 2022')]\n",
    "extraVHRQ2 = int((waVHRZoneQ2['Report_RoomsRented'] / 3).round(0).iloc[0])\n",
    "waVHRZoneQ3 = dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc.RoomType == 'VHR') & (dfOcc.Timeframe == 'Q3 2022')]\n",
    "extraVHRQ3 = int((waVHRZoneQ3['Report_RoomsRented'] / 3).round(0).iloc[0])\n",
    "\n",
    "# add the extra VHR rooms rented to the monthly rows that fall within that quarter Zone_ID is Washoe County and Temporal_Scale is Monthly\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly') \n",
    "              & (dfOcc.RoomType =='VHR') & (dfOcc.Timeframe == 'June'), \n",
    "              'Report_RoomsRented'] = dfOcc['Report_RoomsRented'] + extraVHRQ2\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Washoe County') & (dfOcc['Temporal_Scale'] == 'Monthly')\n",
    "              & (dfOcc.RoomType =='VHR') & (dfOcc.Timeframe.isin(['July', 'August'])),\n",
    "              'Report_RoomsRented'] = dfOcc['Report_RoomsRented'] + extraVHRQ3\n",
    "\n",
    "# if the Zone_ID is Washoe County set Report_OccRate to Report_RoomsRented by Report _RoomsAvailable\n",
    "dfOcc.loc[dfOcc['Zone_ID'] == 'Washoe County', 'Report_OccRate'] = dfOcc['Report_RoomsRented']/dfOcc['Report_RoomsAvailable']\n",
    "\n",
    "## Fill in Missing Data for El Dorado County ##\n",
    "\n",
    "# get total VHRs in El Dorado County from the parcel layer\n",
    "vhrEL = sdfParcel.loc[(sdfParcel.JURISDICTION == 'EL') & (sdfParcel.VHR == 'Yes'), 'APN'].count()\n",
    "\n",
    "# caclulate Rooms available for VHRs in El Dorado County using total VHRs from the parcel layer\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Rest of El Dorado County') & (dfOcc['Temporal_Scale'] == 'Monthly') & \n",
    "              (dfOcc.RoomType == 'VHR') & (dfOcc['Period'].isin(['July', 'August'])), \n",
    "              'Report_RoomsAvailable'] = vhrEL * 31\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Rest of El Dorado County') & (dfOcc['Temporal_Scale'] == 'Monthly') &\n",
    "              (dfOcc.RoomType == 'VHR') & (dfOcc['Period'].isin(['June', 'September'])),\n",
    "              'Report_RoomsAvailable'] = vhrEL * 30\n",
    "\n",
    "# calculate Rooms Rented for VHRs in El Dorado County using Report_OccRate and Report_RoomsAvailable\n",
    "dfOcc.loc[(dfOcc['Zone_ID'] == 'Rest of El Dorado County') & (dfOcc['Temporal_Scale'] == 'Monthly') & (dfOcc.RoomType == 'VHR'), \n",
    "              'Report_RoomsRented'] = (dfOcc['Report_OccRate'] * dfOcc['Report_RoomsAvailable']).fillna(0).astype(int)\n",
    "\n",
    "## Calculate Weighted Average Occupancy Rate ##\n",
    "\n",
    "# df copy\n",
    "df = dfOcc.copy()\n",
    "\n",
    "# Define the weights for each month based on the number of days they contribute\n",
    "weights = {\n",
    "    'June'          : 8/20,\n",
    "    'August'        : 3/20,\n",
    "    'September'     : 9/20,\n",
    "    'April-June'    : 8/20,\n",
    "    'July-September': 12/20\n",
    "}\n",
    "\n",
    "# calculate the weighted occupancy rates\n",
    "for key,value in weights.items():\n",
    "    # Apply weights to the occupancy rates\n",
    "    df.loc[df['Period'] == key, 'TRPA_OccRate'] = df['Report_OccRate'] * value\n",
    "\n",
    "# Calculate RoomsRentedPerDay based on the period\n",
    "df['RoomsRentedPerDay'] = df.apply(lambda row: row['Report_RoomsRented'] / 30 if row['Period'] in ['June', 'September'] else\n",
    "                                   (row['Report_RoomsRented'] / 31 if row['Period'] == 'August' else\n",
    "                                    (row['Report_RoomsRented'] / 91 if row['Period'] == 'April-June' else\n",
    "                                     (row['Report_RoomsRented'] / 92 if row['Period'] == 'July-September' else 0))), axis=1).fillna(0).astype(int)\n",
    "\n",
    "# filter by Temporal_Scale\n",
    "df_monthly   = df.loc[df['Temporal_Scale'] == 'Monthly']\n",
    "df_quarterly = df.loc[df['Temporal_Scale'] == 'Quarterly']\n",
    "\n",
    "# group by for montthly and quarterly and mean for Report_OccRate and sum for TRPA_OccRate\n",
    "dfMonthly   = df_monthly.groupby(['Zone_ID', 'RoomType', 'Temporal_Scale']).agg({'RoomsRentedPerDay': 'mean','Report_RoomsAvailable':'sum',\n",
    "                                                                                 'Report_RoomsRented':'sum', 'Report_OccRate': 'mean', \n",
    "                                                                                 'TRPA_OccRate': 'sum'}).reset_index()\n",
    "\n",
    "dfQuarterly = df_quarterly.groupby(['Zone_ID', 'RoomType', 'Temporal_Scale']).agg({'RoomsRentedPerDay': 'mean','Report_RoomsAvailable':'sum',\n",
    "                                                                                   'Report_RoomsRented':'sum', 'Report_OccRate': 'mean', \n",
    "                                                                                   'TRPA_OccRate': 'sum'}).reset_index()\n",
    "\n",
    "# concat the two dataframes into the final occupancy rate dataframe\n",
    "dfOccFinal = pd.concat([dfMonthly, dfQuarterly]).reset_index(drop=True)\n",
    "\n",
    "# cast RoomsRentedPerDay as int \n",
    "dfOccFinal['RoomsRentedPerDay'] = dfOccFinal['RoomsRentedPerDay'].astype(int)\n",
    "# drop rows where Zone_ID is Washoe County and Temporal_Scale is Quarterly\n",
    "df = dfOccFinal.loc[~((dfOccFinal['Zone_ID'] == 'Washoe County') & (dfOccFinal['Temporal_Scale'] == 'Quarterly'))].reset_index(drop=True)\n",
    "df.info()\n",
    "# save to pickle\n",
    "df.to_pickle(out_dir / 'occupancy_rates.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> general spatial joins and categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join to get TAZ\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_taz, \"Existing_Development_TAZ\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Block Group\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_block, \"Existing_Development_BlockGroup\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatail join to get Occupancy Rate Zone\n",
    "sdf_occ = sdf_occ.loc[sdf_occ['OccupancyRate_ZoneID'] != 'CSLT_ALL']\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_occ, \"Existing_Development_OccupancyZone\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Parcels APN with TAU Types\n",
    "tau_lookup = pd.read_csv('Lookup_Lists/lookup_tau_type.csv')\n",
    "#sro_lookup = pd.read_csv('Lookup_Lists/lookup_sro.csv')\n",
    "\n",
    "# check if fields exist in the dataframes\n",
    "sdfParcel   = check_field(sdf_units, final_schema)\n",
    "\n",
    "# merge parcel 2022 with parcel VHR\n",
    "sdfParcel = sdfParcel.merge(sdf_vhr, on='APN', how='left', indicator=True)\n",
    "\n",
    "# calculate VHR = Yes if VHR is in the parcel\n",
    "sdfParcel['VHR'] = 'No'\n",
    "sdfParcel.loc[sdfParcel['_merge'] == 'both', 'VHR'] = 'Yes'\n",
    "\n",
    "# setup TAU_Type\n",
    "sdfParcel['TAU_TYPE'] = 'N/A'\n",
    "\n",
    "# filter parcels so only APNs in the lookup are included\n",
    "sdfTAU = sdfParcel[sdfParcel['APN'].isin(tau_lookup['APN'])]\n",
    "# get TAU_Type from lookup\n",
    "sdfTAU['TAU_TYPE'] = sdfTAU['APN'].map(tau_lookup.set_index('APN')['TAU_Type'])\n",
    "\n",
    "# any row with ToursitAccommodation_Units > 0 and TAU_Type is null, set TAU_Type to 'HotelMotel'\n",
    "sdfParcel.loc[(sdfParcel['TouristAccommodation_Units'] > 0) & (sdfParcel['TAU_TYPE']=='N/A'), 'TAU_TYPE'] = 'HotelMotel'\n",
    "# for the rows in df that match rows by APN in dfTAU set TAU_Type to the value in dfTAU\n",
    "sdfParcel.loc[sdfParcel['APN'].isin(sdfTAU['APN']), 'TAU_TYPE'] = sdfTAU['TAU_TYPE']\n",
    "\n",
    "# remove _x from column names\n",
    "sdfParcel.columns = sdfParcel.columns.str.replace('_x', '')\n",
    "\n",
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_units_taz   = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_TAZ\", sr=sr)  \n",
    "sdf_units_block = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_BlockGroup\", sr=sr)\n",
    "sdf_units_occ   = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_OccupancyZone\", sr=sr)\n",
    "\n",
    "# map dictionary to sdf_units dataframe to fill in TAZ and Block Group fields\n",
    "sdfParcel['TAZ']           = sdfParcel.APN.map(dict(zip(sdf_units_taz.APN,   sdf_units_taz.TAZ)))\n",
    "sdfParcel['BLOCK_GROUP']   = sdfParcel.APN.map(dict(zip(sdf_units_block.APN, sdf_units_block.TRPAID)))\n",
    "sdfParcel['OCCUPANCY_ZONE']= sdfParcel.APN.map(dict(zip(sdf_units_occ.APN,   sdf_units_occ.OccupancyRate_ZoneID)))\n",
    "\n",
    "# if df.JURISDICTION == \"CSLT\" and VHR == \"Yes\" then set OCCUPANCY_ZONE to \"CSLT_ALL\"\n",
    "sdfParcel.loc[(sdfParcel['JURISDICTION'] == 'CSLT') & (sdfParcel['VHR'] == 'Yes'), 'OCCUPANCY_ZONE'] = 'CSLT_ALL'\n",
    "\n",
    "# columns to keep\n",
    "sdfParcel = sdfParcel[final_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle and save to feature class\n",
    "outfc = 'sdf_units_attributed'\n",
    "# export to feature class\n",
    "sdfParcel.spatial.to_featureclass(location=os.path.join(gdb, outfc), sanitize_columns=False)\n",
    "# export to pickle\n",
    "sdfParcel.to_pickle(sdfParcel_pickle_part1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spatial join to apply Lodging Occupany Rates to parcel layer, \n",
    "* select parcels where Lodging Occupancy Rate is Null, \n",
    "* run interpolation, \n",
    "* apply interpoleted values to parcels where occupancy rate is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the pickled parcel dataframe\n",
    "sdfParcel = pd.read_pickle(sdfParcel_pickle_part1)\n",
    "# read in the pickled occupancy rates table\n",
    "dfOcc = pd.read_pickle(occupancy_rates_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Filter Occupancy Rate table to Timeframe and Room Type, Merge with Occupancy Zone Feature Class, and Export to Feature Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filter occupancy rate table by RoomType\n",
    "dfOccTAU = dfOcc.loc[dfOcc['RoomType'].isin(['HotelMotel', 'Casino', 'Resort'])]    \n",
    "dfOccVHR = dfOcc.loc[dfOcc['RoomType'] == 'VHR']\n",
    "# specify the output feature classes\n",
    "tau_occ_zones = os.path.join(gdb,'OccupancyRate_Zones_TAU')\n",
    "vhr_occ_zones = os.path.join(gdb,'OccupancyRate_Zones_VHR')\n",
    "# merge occupancy rate data to occupancy zones\n",
    "sdfTAU= pd.merge(sdf_occ, dfOccTAU, left_on='OccupancyRate_ZoneID', right_on='Zone_ID', how='left')\n",
    "# export sdf to feature class\n",
    "sdfTAU.spatial.to_featureclass(location=tau_occ_zones, overwrite=True)\n",
    "# merge occupancy rate data to occupancy zones\n",
    "sdfVHR = pd.merge(sdf_occ, dfOccVHR, left_on='OccupancyRate_ZoneID', right_on='Zone_ID', how='left')\n",
    "# export sdf to feature class put in memory workspace\n",
    "sdfVHR.spatial.to_featureclass(location=vhr_occ_zones, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Join TAU and VHR parcels to occupancy rate zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rows where VHR = Yes and rows where TouristAccommodation_Units > 0\n",
    "sdfTAU = sdfParcel.loc[sdfParcel['TouristAccommodation_Units'] > 0]\n",
    "sdfVHR = sdfParcel.loc[sdfParcel['VHR'] == 'Yes']\n",
    "# specify the output feature classes\n",
    "tau_occ_zones = os.path.join('Workspace.gdb','OccupancyRate_Zones_TAU')\n",
    "vhr_occ_zones = os.path.join('Workspace.gdb','OccupancyRate_Zones_VHR')\n",
    "\n",
    "# spatial join TAU to occupancy rate zones with TAU values\n",
    "spjoin_tau = arcpy.analysis.SpatialJoin(sdfTAU, tau_occ_zones, 'OccupancyRate_Zones_TAU_Parcels', \n",
    "                                        \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", None, \"INTERSECT\", None, None)\n",
    "# spatial join VHR to occupancy rate zones with VHR values\n",
    "spjoin_vhr = arcpy.analysis.SpatialJoin(sdfVHR, vhr_occ_zones, 'OccupancyRate_Zones_VHR_Parcels', \n",
    "                                        \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", None, \"INTERSECT\", None, None)\n",
    "\n",
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_parcel_tau_rates   = pd.DataFrame.spatial.from_featureclass(\"OccupancyRate_Zones_TAU_Parcels\", sr=sr)  \n",
    "sdf_parcel_vhr_rates = pd.DataFrame.spatial.from_featureclass(\"OccupancyRate_Zones_VHR_Parcels\", sr=sr)\n",
    "\n",
    "# map dictionary for TAU and VHR parcels respectively\n",
    "sdfParcel['VHR_Lodging_Occupancy_Rate']   = sdfVHR.APN.map(dict(zip(sdf_parcel_vhr_rates.APN,   sdf_parcel_vhr_rates.trpa_occ_rate)))\n",
    "sdfParcel['TAU_Lodging_Occupancy_Rate']   = sdfTAU.APN.map(dict(zip(sdf_parcel_tau_rates.APN,   sdf_parcel_tau_rates.trpa_occ_rate)))\n",
    "\n",
    "# cast VHR_Lodging_Occupancy_Rate and TAU_Lodging_Occupancy_Rate as float and fill na as 0\n",
    "sdfParcel['VHR_Lodging_Occupancy_Rate'] = sdfParcel['VHR_Lodging_Occupancy_Rate'].fillna(0).astype(float)\n",
    "sdfParcel['TAU_Lodging_Occupancy_Rate'] = sdfParcel['TAU_Lodging_Occupancy_Rate'].fillna(0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle and save to feature class\n",
    "outfc = 'sdf_units_attributed_occupancy'\n",
    "# export to feature class\n",
    "sdfParcel.spatial.to_featureclass(location=os.path.join(gdb, outfc), sanitize_columns=False)\n",
    "# export to pickle\n",
    "sdfParcel.to_pickle(sdfParcel_pickle_part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill in parcel level missing occupancy rates with interpolated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pickle\n",
    "sdfParcel = pd.read_pickle(sdfParcel_pickle_part2)\n",
    "sdfParcel.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generate Spatial Interpolated Occupancy Rate Surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the extent environment using a feature class\n",
    "arcpy.env.extent = os.path.join(gdb,\"OccupancyRate_Zones_TAU\")\n",
    "# set the input feature class\n",
    "tau_fc = os.path.join(gdb,'TAU_points')\n",
    "vhr_fc = os.path.join(gdb,'VHR_points')\n",
    "# set the output raster\n",
    "tau_raster = os.path.join(gdb,'tau_occupancy_rate')\n",
    "vhr_raster = os.path.join(gdb,'vhr_occupancy_rate')\n",
    "# set the output cell size\n",
    "cell_size = 30\n",
    "# set the power parameter\n",
    "power = 2\n",
    "# set the search radius\n",
    "search_radius = 10000\n",
    "\n",
    "# select rows where TAU_TYPE is not null but TAU_Occupancy_Rate is null\n",
    "tauParcel_NULLocc = sdfParcel.loc[(sdfParcel['TAU_TYPE'].isin(['HotelMotel','Casino','Resort'])) & (sdfParcel['TAU_Lodging_Occupancy_Rate']==0)]\n",
    "tauParcel_NULLocc.spatial.to_featureclass(location=os.path.join(gdb,\"TAU_NULL_occ\"), overwrite=True)\n",
    "vhrParcel_NULLocc = sdfParcel.loc[(sdfParcel['VHR'] == 'Yes') & (sdfParcel['VHR_Lodging_Occupancy_Rate']==0)] \n",
    "vhrParcel_NULLocc.spatial.to_featureclass(location=os.path.join(gdb,\"VHR_NULL_occ\"), overwrite=True)\n",
    "\n",
    "# get not null parcels for TAU and VHR\n",
    "tauParcel_notNULL = sdfParcel.loc[(sdfParcel['TAU_TYPE'].isin(['HotelMotel','Casino','Resort'])) & (sdfParcel['TAU_Lodging_Occupancy_Rate']!=0)]\n",
    "tauParcel_notNULL.spatial.to_featureclass(location=os.path.join(gdb,\"TAU_occ\"), overwrite=True)\n",
    "vhrParcel_notNULL = sdfParcel.loc[(sdfParcel['VHR'] == 'Yes') & (sdfParcel['VHR_Lodging_Occupancy_Rate']!=0)]\n",
    "vhrParcel_notNULL.spatial.to_featureclass(location=os.path.join(gdb,\"VHR_occ\"), overwrite=True)\n",
    "\n",
    "# feature to point for TAU and VHR\n",
    "arcpy.management.FeatureToPoint(tauParcel_NULLocc, os.path.join(gdb,'TAU_NULL_points'),\"INSIDE\")\n",
    "arcpy.management.FeatureToPoint(vhrParcel_NULLocc, os.path.join(gdb, 'VHR_NULL_points'), \"INSIDE\")\n",
    "arcpy.management.FeatureToPoint(tauParcel_notNULL, os.path.join(gdb,'TAU_points'), \"INSIDE\")\n",
    "arcpy.management.FeatureToPoint(vhrParcel_notNULL, os.path.join(gdb,'VHR_points'), \"INSIDE\")\n",
    "\n",
    "# run the IDW for TAU parcels with rates\n",
    "arcpy.sa.Idw(tau_fc, \n",
    "            z_field='TAU_Lodging_Occupancy_Rate', \n",
    "            cell_size=cell_size, \n",
    "            power=power, \n",
    "            search_radius=search_radius).save(tau_raster)\n",
    "# and for VHR parcels with rates\n",
    "arcpy.sa.Idw(vhr_fc,\n",
    "            z_field='VHR_Lodging_Occupancy_Rate',\n",
    "            cell_size=cell_size,\n",
    "            power=power,\n",
    "            search_radius=search_radius).save(vhr_raster)\n",
    "\n",
    "# Set the local variables for ZonalStatisticsAsTable\n",
    "zoneField = \"APN\"\n",
    "tauZoneData = os.path.join(gdb, 'TAU_NULL_occ')\n",
    "vhrZoneData = os.path.join(gdb, 'VHR_NULL_occ')\n",
    "tauValueRaster = os.path.join(gdb,'tau_occupancy_rate')\n",
    "vhrValueRaster = os.path.join(gdb,'vhr_occupancy_rate')\n",
    "tauTable = os.path.join(gdb, \"zonalstat_TAU_Occupancy\")\n",
    "vhrTable = os.path.join(gdb, \"zonalstat_VHR_Occupancy\")\n",
    "\n",
    "# Execute ZonalStatisticsAsTable\n",
    "tauZSaT = arcpy.sa.ZonalStatisticsAsTable(tauZoneData, zoneField, tauValueRaster, \n",
    "                                            tauTable, \"DATA\", \"MEAN\")\n",
    "vhrZSaT = arcpy.sa.ZonalStatisticsAsTable(vhrZoneData, zoneField, vhrValueRaster,\n",
    "                                            vhrTable, \"DATA\", \"MEAN\")\n",
    "\n",
    "# convert to dataframe\n",
    "tauZonalStats = arcpy.da.TableToNumPyArray(tauZSaT, '*')\n",
    "vhrZonalStats = arcpy.da.TableToNumPyArray(vhrZSaT, '*')\n",
    "dfTAU = pd.DataFrame(tauZonalStats)\n",
    "dfVHR = pd.DataFrame(vhrZonalStats)\n",
    "\n",
    "# Create a temporary column with the new mapped values\n",
    "sdfParcel['New_TAU_Lodging_Occupancy_Rate'] = sdfParcel['APN'].map(dict(zip(dfTAU['apn'], dfTAU['MEAN'])))\n",
    "sdfParcel['New_VHR_Lodging_Occupancy_Rate'] = sdfParcel['APN'].map(dict(zip(dfVHR['apn'], dfVHR['MEAN'])))\n",
    "\n",
    "# Combine the new column with the existing column, preserving existing values where the new values are NaN\n",
    "sdfParcel['TAU_Lodging_Occupancy_Rate'] = sdfParcel['New_TAU_Lodging_Occupancy_Rate'].combine_first(sdfParcel['TAU_Lodging_Occupancy_Rate'])\n",
    "sdfParcel['VHR_Lodging_Occupancy_Rate'] = sdfParcel['New_VHR_Lodging_Occupancy_Rate'].combine_first(sdfParcel['VHR_Lodging_Occupancy_Rate'])\n",
    "\n",
    "# Drop the temporary column\n",
    "sdfParcel.drop(columns=['New_TAU_Lodging_Occupancy_Rate'], inplace=True)\n",
    "sdfParcel.drop(columns=['New_VHR_Lodging_Occupancy_Rate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Why isnt the zonal stats working for these parcels?? ###\n",
    "# # are there any sdfParcel rows where TAU_Lodging_Occupancy_Rate is 0 and ToursitAccommodation_Units > 0\n",
    "# sdfParcel.loc[(sdfParcel['TAU_Lodging_Occupancy_Rate'] == 0) & (sdfParcel['TouristAccommodation_Units'] > 0)]\n",
    "# those APNs to list\n",
    "apn_list = sdfParcel.loc[(sdfParcel['TAU_Lodging_Occupancy_Rate'] == 0) & (sdfParcel['TouristAccommodation_Units'] > 0)]['APN'].tolist()\n",
    "# classify the occupancy rates for those parcels\n",
    "sdfParcel.loc[sdfParcel['APN'].isin(apn_list), 'TAU_Lodging_Occupancy_Rate'] = 0.592337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vhr_apn_list = sdfParcel.loc[(sdfParcel['VHR_Lodging_Occupancy_Rate'] == 0) & (sdfParcel['VHR'] == 'Yes')]['APN'].tolist()\n",
    "# sdfParcel.loc[sdfParcel['APN'].isin(vhr_apn_list), 'VHR_Lodging_Occupancy_Rate'] = 0.592337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle and save to feature class\n",
    "outfc = 'sdf_units_attributed_occupancy_interpolated'\n",
    "# export to feature class\n",
    "sdfParcel.spatial.to_featureclass(location=os.path.join(gdb, outfc), sanitize_columns=False)\n",
    "# export to pickle\n",
    "sdfParcel.to_pickle(sdfParcel_pickle_part3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Campgrounds - seperate deal..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Campground Occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge campground data with occupancy rate data on campground name\n",
    "dfCampOcc = sdf_campground.merge(dfCamp, left_on='RECREATION_NAME', right_on='Campground', \n",
    "                                      how='left', indicator=True)\n",
    "\n",
    "# spatial join TAZ data to campground data\n",
    "arcpy.SpatialJoin_analysis(dfCampOcc, sdf_taz, 'taz_campground', \n",
    "                           'JOIN_ONE_TO_ONE', 'KEEP_ALL', \n",
    "                           match_option='HAVE_THEIR_CENTER_IN')\n",
    "\n",
    "# read in output of spatial join as sdf\n",
    "sdf_campground_taz = pd.DataFrame.spatial.from_featureclass('taz_campground')\n",
    "\n",
    "# get sites sold by multiplying the number of sites by the occupancy rate\n",
    "sdf_campground_taz['SitesSold'] = sdf_campground_taz['Total_Sites'] * sdf_campground_taz['Occupancy_Rate']\n",
    "\n",
    "# group by TAZ and sum of sites sold within TAZ\n",
    "sdf_campground_taz_grouped = sdf_campground_taz.groupby('TAZ').agg(\n",
    "                                                {'SitesSold': 'sum'}).reset_index()\n",
    "\n",
    "# sdf_campground to pickle\n",
    "sdf_campground_taz_grouped.to_pickle(out_dir / 'taz_campground_occupancy.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of campground names\n",
    "campground_names1 = sdf_campground['RECREATION_NAME'].tolist()\n",
    "campground_names2 = dfCamp['Campground'].tolist()\n",
    "\n",
    "# check for names missing in one list\n",
    "print(set(campground_names2).difference(campground_names1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coutns of campgrounds in each list\n",
    "print(len(campground_names1))\n",
    "print(len(campground_names2))\n",
    "# check if unique\n",
    "print(len(set(campground_names1)))\n",
    "print(len(set(campground_names2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows where the campground name is Zephyr Cove Resort RV Campground\n",
    "dfCamp.loc[dfCamp['Campground'] == 'Zephyr Cove RV Campground']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCamp.Campground.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which campgrounds are in the occupancy rate table but not in the campground table\n",
    "campgrounds = set(campground_names1).difference(campground_names2)\n",
    "print(campgrounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCampOcc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_campground_taz_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interpolation to fill nan if neccesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge campground data with occupancy rate data on campground name\n",
    "dfCampOcc = sdf_campground.merge(dfCamp, left_on='RECREATION_NAME', right_on='Campground', \n",
    "                                      how='left', indicator=True)\n",
    "\n",
    "# keep only columns of interest\n",
    "sdf_campground = sdf_campground[['RECREATION_NAME', 'Occupancy_Rate','SHAPE']]\n",
    "\n",
    "# filter sdf_campground to only campgrounds with occupancy rate data\n",
    "sdf_campground = sdf_campground[sdf_campground['Occupancy_Rate'].notnull()]\n",
    "\n",
    "# IDW to get the occupancy rate for each campground\n",
    "# set the output cell size\n",
    "cell_size = 500\n",
    "# set the power parameter\n",
    "power = 2\n",
    "# set the search radius\n",
    "search_radius = 5000\n",
    "# set the output raster\n",
    "out_raster = 'campground_occupancy_rate'\n",
    "# run the IDW\n",
    "arcpy.sa.Idw(in_features=sdf_campground, \n",
    "             z_field='Occupancy_Rate', \n",
    "             cell_size=cell_size, \n",
    "             power=power, \n",
    "             search_radius=search_radius).save(out_raster)\n",
    "\n",
    "# spatial join to campground points with NaN occupancy rate\n",
    "sdf_campground_nan = sdf_campground[sdf_campground['Occupancy_Rate'].isnull()]\n",
    "# spatial join to campground points with NaN occupancy rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_campground_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply weighting by model days (same as occupancy zone rate weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overnight Visitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_units.groupby('TAZ','TAU_TYPE').agg({'TouristAccommodation_Units': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAUs 6190 were rented on model day....2,200 VHRs were rented on model day\n",
    "\n",
    "# get the number of occupied units by multiplying the number of units by the occupancy rate\n",
    "\n",
    "# multiply the number of TAUs by the occupancy rate then add up by zone? or by TAZ?\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentHouseSeasonal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Type to Null\n",
    "sdf_school['TYPE'] = None\n",
    "# set SchoolType to 'elementary' if it contains 'elementary' or 'magnet' or 'academy'\n",
    "sdf_school.loc[sdf_school['NAME'].str.contains('elementary', case=False), 'TYPE'] = 'Elementary School'\n",
    "# set SchoolType to 'middle' if it contains 'middle'\n",
    "sdf_school.loc[sdf_school['NAME'].str.contains('middle', case=False), 'TYPE'] = 'Middle School'\n",
    "# set SchoolType to 'high' if it contains 'high'\n",
    "sdf_school.loc[sdf_school['NAME'].str.contains('high', case=False), 'TYPE'] = 'High School'\n",
    "# set SchoolType to 'college' if it contains 'college'\n",
    "sdf_school.loc[sdf_school['NAME'].str.contains('college', case=False), 'TYPE'] = 'College'\n",
    "# set SchoolType to 'other' if it it does not contain any of the above\n",
    "sdf_school.loc[sdf_school['TYPE'].isnull(), 'TYPE'] = 'Elementary School'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join TAZs to School points\n",
    "sdf_school_taz = sdf_school.spatial.join(sdf_taz, how='inner')\n",
    "# group by TYPE and sum of Enrollment within TAZ \n",
    "sdf_school_taz_grouped = sdf_school_taz.groupby(['TYPE', 'TAZ']).agg(\n",
    "                                                {'ENROLLMENT': 'sum'}).reset_index()\n",
    "# unstack by TYPE as columns and TAZ as a column\n",
    "sdf_school_taz_grouped_pivot = sdf_school_taz_grouped.pivot(index='TAZ', \n",
    "                                                            columns='TYPE', \n",
    "                                                            values='ENROLLMENT').reset_index()\n",
    "# merge to sdf_taz to get all tazs\n",
    "sdf_taz_school = pd.merge(sdf_taz, sdf_school_taz_grouped_pivot, how='left', on='TAZ')\n",
    "\n",
    "# drop SHAPE column\n",
    "sdf_taz_school = sdf_taz_school.drop(columns='SHAPE')\n",
    "# fill NA with 0 for all rows\n",
    "sdf_taz_school = sdf_taz_school.fillna(0)\n",
    "# cast all fields to int\n",
    "sdf_taz_school = sdf_taz_school.astype(int)\n",
    "# rename columns\n",
    "sdf_taz_school.rename(columns={'Elementary School':'elementary_school_enrollment',\n",
    "                               'Middle School':'middle_school_enrollment',\n",
    "                               'High School':'high_school_enrollment',\n",
    "                               'College':'college_enrollment'}, inplace=True)\n",
    "\n",
    "# export to csv\n",
    "sdf_taz_school.to_csv(os.path.join('SchoolEnrollment.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socio Econ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Census data\n",
    "census_url = 'https://maps.trpa.org/server/rest/services/Demographics/MapServer/28'\n",
    "df_census = get_fs_data(census_url)\n",
    "df_census_2022 = df_census[(df_census['year_sample'] == 2022) & (df_census['sample_level'] == 'block group')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant census variables and calculate rates at block group level\n",
    "# Get Occupancy Data - B25002_003E = Vacant, B25002_002E = Occupied , B25004_006E = Vacant Seasonal\n",
    "occupancy_codes = ['B25002_003E','B25002_002E', 'B25004_006E']\n",
    "df_census_occupancy = df_census_2022[df_census_2022['variable_code'].isin(occupancy_codes)]\n",
    "df_census_occupancy = df_census_occupancy[['TRPAID', 'variable_code', 'value']]\n",
    "# pivot to wide format so we can calculate percentages and totals\n",
    "df_census_occupancy = df_census_occupancy.pivot(index='TRPAID', columns='variable_code', values='value').reset_index()\n",
    "# vacant units + occupied units = total units\n",
    "df_census_occupancy['total_units'] = df_census_occupancy['B25002_003E'] + df_census_occupancy['B25002_002E']\n",
    "# occupancy rate = occupied units / total units\n",
    "df_census_occupancy['occupancy_rate'] = df_census_occupancy['B25002_002E'] / df_census_occupancy['total_units']\n",
    "# seasonal rate = seasonal units / total units\n",
    "df_census_occupancy['seasonal_rate'] = df_census_occupancy['B25004_006E'] / df_census_occupancy['total_units']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Household Size Data - B25010_001E = Total Households\n",
    "df_census_household_size = df_census_2022[df_census_2022['variable_code'] == 'B25010_001E']\n",
    "df_census_household_size = df_census_household_size[['TRPAID', 'variable_code', 'value']]\n",
    "df_census_household_size = df_census_household_size.pivot(index='TRPAID', columns='variable_code', values='value').reset_index()\n",
    "df_census_household_size['household_size'] = df_census_household_size['B25010_001E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Codes by the category they fall into - Census categroy to broader category\n",
    "code_lookup = pd.read_csv('Lookup_Lists/occupancy_census_codes.csv')\n",
    "#Filter census so only variable codes in the code lookup are included\n",
    "df_census_income = df_census_2022[df_census_2022['variable_code'].isin(code_lookup['variable_code'])]\n",
    "#Create a new column that has a value from code lookup based on the variable code\n",
    "df_census_income['income_category'] = df_census_income['variable_code'].map(code_lookup.set_index('variable_code')['category'])\n",
    "#group by block group and income category and sum the values\n",
    "df_census_income = df_census_income.groupby(['TRPAID','income_category'])['value'].sum().reset_index()\n",
    "df_census_income = df_census_income.pivot(index='TRPAID', columns='income_category', values='value').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRPAID is a 16 digit ID, but it is imported as a float. Convert to string and to retain leading zeros\n",
    "df_census_household_size['TRPAID']= df_census_household_size['TRPAID'].astype(str).str.zfill(16)\n",
    "df_census_income['TRPAID']= df_census_income['TRPAID'].astype(str).str.zfill(16)\n",
    "# merge all the census data together\n",
    "df_census_occupancy_all = pd.merge(df_census_occupancy, df_census_household_size, on='TRPAID', how='left')\n",
    "df_census_all = pd.merge(df_census_occupancy_all, df_census_income, on='TRPAID', how='left')\n",
    "# rename columns of df_census_all\n",
    "column_rename = {\n",
    "    'B25002_003E': 'vacant_units',\n",
    "    'B25002_002E': 'occupied_units',\n",
    "    'B25004_006E': 'seasonal_units',\n",
    "    'High Income': 'high_income',\n",
    "    'Low Income': 'low_income',\n",
    "    'Medium Income': 'middle_income',\n",
    "}\n",
    "df_census_all.rename(columns=column_rename, inplace=True)\n",
    "\n",
    "df_census_all.drop(columns=['B25010_001E'], inplace=True)\n",
    "# calculate proportions of income categories\n",
    "df_census_all['high_income_proportion'] = df_census_all['high_income'] / df_census_all['occupied_units']\n",
    "df_census_all['middle_income_proportion'] = df_census_all['middle_income'] / df_census_all['occupied_units']\n",
    "df_census_all['low_income_proportion'] = df_census_all['low_income'] / df_census_all['occupied_units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a block group level number of units and vhrs so we can get to adjusted seasonal rates\n",
    "df_units_block_grouped = sdf_units_block.groupby('TRPAID').agg({'Residential_Units': 'sum'}).reset_index()\n",
    "df_vhr_grouped     = sdf_vhr_block.groupby('TRPAID').agg({'APN': 'count'}).reset_index()\n",
    "# merge the two dataframes\n",
    "df_units_vhr = pd.merge(df_units_block_grouped, df_vhr_grouped, on='TRPAID', how='left')\n",
    "#rename APN to VHR\n",
    "df_units_vhr.rename(columns={'APN':'VHR'}, inplace=True)\n",
    "# fill in missing values with 0\n",
    "df_units_vhr = df_units_vhr.fillna(0)\n",
    "df_units_vhr['non_vhr_units'] = df_units_vhr['Residential_Units'] - df_units_vhr['VHR']\n",
    "# join this to the census data \n",
    "df_census_vhr = pd.merge(df_census_all, df_units_vhr, on='TRPAID', how='left')\n",
    "# calculate the non-adjusted number of seasonal units and then subtract the number of VHRs\n",
    "df_census_vhr['non_adjusted_seasonal_units'] = df_census_vhr['seasonal_rate']*df_census_vhr['Residential_Units']\n",
    "df_census_vhr['adjusted_seasonal_units'] = df_census_vhr['non_adjusted_seasonal_units'] - df_census_vhr['VHR']\n",
    "# Manually adjust the seasonal units for block group 3200500170022020 because of a lag in the data\n",
    "# The census reports 100% occupancy but I think it has to do with the beach club development\n",
    "df_census_vhr.loc[df_census_vhr['TRPAID'] == '3200500170022020', 'adjusted_seasonal_units'] = 0\n",
    "# calculate the adjusted seasonal rate\n",
    "df_census_vhr['adjusted_seasonal_rate'] = df_census_vhr['adjusted_seasonal_units'] / df_census_vhr['Residential_Units']\n",
    "# rename a final database\n",
    "df_census_final = df_census_vhr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a parcel layer with all of the census rates applied by merging df_census_final with sdf_units_block\n",
    "sdf_units_census_values = pd.merge(sdf_units_block, df_census_final, on='TRPAID', how='left')\n",
    "#This can eventually be grouped at the TAZ level or joined to the master parcel dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top line employment data for NV from 2018 lives here: ????\n",
    "# we got employment data from NV at the Tahoe Basin level by NAICS code....\n",
    "\n",
    "# get the employment data\n",
    "nv_employ = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE HAVE GOOD DATA FOR CASINO EMPLOYMENT on the South Shore ## \n",
    "# for employment data we have multiple years of CA EDD data\n",
    "# california employment development department data for 2018 and 2022 was transformed to a feature class and spatial joined to TAZs and Block Group\n",
    "# exported to a csv\n",
    "# stacekd data by temporal scale\n",
    "# grouped by TAZ and NAICS code, and summed employment\n",
    "# F:\\GIS\\PROJECTS\\ResearchAnalysis\\Employment\\Data\\EDD_Grouped\n",
    "# F:\\GIS\\PROJECTS\\ResearchAnalysis\\Employment\\\n",
    "# then looking at difference of total and trends over time (month-month) and year over year\n",
    "#\n",
    "# LODES data https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/142\n",
    "\n",
    "# compare 2018 to 2022 by block group \n",
    "\n",
    "# checking trends of each. \n",
    "#  - what is the trend of employment by NAICS code\n",
    "#  - what is the trend of employment by TAZ\n",
    "#  - what is the trend of employment by block group\n",
    "#  - what is the trend of employment by zip code\n",
    "# \n",
    "# CBP data for 2018 and 2022\n",
    "# data is mostly in the service. or in Vector.sde>Census>Jobs\n",
    "# look at comparisons of trends by same geography and temporal scale\n",
    "\n",
    "# workflow is to get the data, clean it, join it to the spatial data, then group by the spatial data and sum the employment\n",
    "# \n",
    "# establish trends for CA for the three datasources...compare the trends and see if they are similar\n",
    "# \n",
    "\n",
    "### NAICS codes are one order higher in LODES data, CA EDD and CBP data have the same granularity of NAICS codes\n",
    "### LODES is by year so the trend might be different if there is a sesaonal component to the data\n",
    "\n",
    "# we'll have two of the three datasets analyzed for Nevada and all three in California.\n",
    "    # where we have all three datasets we'll compare the trends and see if they are similar\n",
    "    # we'll look at the trends for each dataset and see if they are similar\n",
    "    # we'll look at the trends for each geography and see if they are similar\n",
    "    # we'll look at the trends for each temporal scale and see if they are similar\n",
    "\n",
    "# For Nevada we have block level data for 2018 so if consistent with 2022 we can use that as a proxy for 2022\n",
    "\n",
    "# we subtract out any known employment from the 2018 data (e.g. Lakeside Inn) and compare the trends\n",
    "# generate adjustment factors by sector and apply those adjustments to the 2018 data that was aggregated to the TAZ level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAZ Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Scaling Factor Data Engineering\n",
    "\n",
    "> Needs\n",
    "\n",
    "* Place.ai data by Jurisdiction ?\n",
    "\n",
    "* Model day(s) weighting logic\n",
    "    * will we need July data? \n",
    "\n",
    "* for quarterly data by zone\n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basin Summary Category\n",
    "* lodging occupancy rate\n",
    "* campground occupancy rate\n",
    "* house(VHR) rate\n",
    "* seasonal rate\n",
    "* lodging unit\n",
    "* campground\n",
    "* percentHouseSeasonal\n",
    "* school enrollment\n",
    "* employment\n",
    "* residential unit\n",
    "* total persons\n",
    "* census occupancy rate\n",
    "* low income res unit\n",
    "* medium income res unit\n",
    "* high income res unit\n",
    "* total occupied unit\n",
    "* persons per occupied unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
