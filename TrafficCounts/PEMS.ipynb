{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a .gz file and return a DataFrame\n",
    "def read_gz_to_dataframe(file_path, columnnames=None):\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        df = pd.read_csv(f, header=None, names=columnnames)\n",
    "    return df\n",
    "\n",
    "# Function to load and concatenate all .gz files in a folder\n",
    "def load_and_concatenate_files(folder_path, column_names=None):\n",
    "    dataframes = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.gz'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = read_gz_to_dataframe(file_path, column_names)\n",
    "            dataframes.append(df)\n",
    "    concatenated_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return concatenated_df\n",
    "\n",
    "\n",
    "# Function to filter data and produce the summary table\n",
    "def summarize_flow(df, confidence_level, timestamp_col, percent_observed_col, total_flow_col):\n",
    "    # Ensure the timestamp is a datetime object\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "    \n",
    "    # Filter the DataFrame by the specified confidence level\n",
    "    filtered_df = df[df[percent_observed_col] >= confidence_level]\n",
    "    \n",
    "    # Add columns for the year, month, and day of the week\n",
    "    filtered_df['year'] = filtered_df[timestamp_col].dt.year\n",
    "    filtered_df['month'] = filtered_df[timestamp_col].dt.month\n",
    "    filtered_df['day_of_week'] = filtered_df[timestamp_col].dt.day_name()\n",
    "    \n",
    "    # Group by the station, year, month, and day of the week\n",
    "    summary = filtered_df.groupby(['Station', 'year', 'month', 'day_of_week']).agg(\n",
    "        average_flow=(total_flow_col, 'mean'),\n",
    "        observed_days=(timestamp_col, 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Sort by station, year, month, and day of the week\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    summary['day_of_week'] = pd.Categorical(summary['day_of_week'], categories=day_order, ordered=True)\n",
    "    summary = summary.sort_values(['Station', 'year', 'month', 'day_of_week'])\n",
    "    summary['confidence_level'] = confidence_level\n",
    "    \n",
    "    return summary\n",
    "# Example usage\n",
    "columnnames = ['Timestamp',\t'Station',\t'District',\t'Route',\t'Direction of Travel',\t'Lane Type',\t'Station Length',\t'Samples',\t'Percent_Observed',\t'total_flow',\t'Delay (V_t=35)',\t'Delay (V_t=40)',\t'Delay (V_t=45)',\t'Delay (V_t=50)',\t'Delay (V_t=55)',\t'Delay (V_t=60)']\n",
    "folder_path = r'F:\\Research and Analysis\\Transportation\\TrafficCounts\\PEMS\\Daily_Counts'\n",
    "df = load_and_concatenate_files(folder_path, columnnames)\n",
    "\n",
    "# Specify the column names and confidence level\n",
    "timestamp_col = 'Timestamp'\n",
    "percent_observed_col = 'Percent_Observed'\n",
    "total_flow_col = 'total_flow'\n",
    "confidence_level_75 = 75\n",
    "confidence_level_25 = 25\n",
    "# Produce the summary table\n",
    "summary_table_75 = summarize_flow(df, confidence_level_75, timestamp_col, percent_observed_col, total_flow_col)\n",
    "summary_table_25 = summarize_flow(df, confidence_level_25, timestamp_col, percent_observed_col, total_flow_col)\n",
    "# Display the summary table\n",
    "summary_table = pd.concat([summary_table_75, summary_table_25])\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table.to_csv(r'F:\\Research and Analysis\\Transportation\\TrafficCounts\\PEMS\\summary_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_metadata = pd.read_csv(r'F:\\Research and Analysis\\Transportation\\TrafficCounts\\PEMS\\d03_text_meta_2024_06_04.txt', sep='\\t')\n",
    "station_metadata.to_csv(r'F:\\Research and Analysis\\Transportation\\TrafficCounts\\PEMS\\station_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Thursday, June 6, 2024 2:28:39 PM\",\"WARNING 040401: NULL geometry ignored. ObjectID = 788, 1744\",\"Succeeded at Thursday, June 6, 2024 2:29:08 PM (Elapsed Time: 29.34 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'F:\\\\Research and Analysis\\\\Transportation\\\\TrafficCounts\\\\PEMS\\\\PEMS_Locations.gdb\\\\station_locations'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pems_geodatabase = r'F:\\Research and Analysis\\Transportation\\TrafficCounts\\PEMS\\PEMS_Locations.gdb'\n",
    "#make a feature class from the station metadata\n",
    "import arcpy\n",
    "arcpy.env.workspace = pems_geodatabase\n",
    "arcpy.management.XYTableToPoint(r'F:\\Research and Analysis\\Transportation\\TrafficCounts\\PEMS\\station_metadata.csv', 'station_locations', 'Longitude', 'Latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
